# ğŸ› ï¸ Orchestration dâ€™un Pipeline Big Data avec Apache Airflow

## ğŸ“Œ Description du Lab

Ce laboratoire pratique a pour objectif de mettre en place un **pipeline Big Data complet**, orchestrÃ© Ã  lâ€™aide dâ€™**Apache Airflow**, simulant un workflow industriel de bout en bout.

Le pipeline couvre toutes les Ã©tapes classiques dâ€™un systÃ¨me Big Data :
- ingestion des donnÃ©es,
- stockage dans un Data Lake,
- transformation,
- chargement dans un Data Lakehouse,
- exploitation analytique.

Lâ€™ensemble est orchestrÃ©, supervisÃ© et contrÃ´lÃ© via lâ€™interface web dâ€™Airflow.

---

## ğŸ¯ Objectifs pÃ©dagogiques

Ã€ la fin de ce lab, vous serez capable de :

- Comprendre un pipeline Big Data end-to-end
- Identifier les rÃ´les du Data Lake et du Data Lakehouse
- ImplÃ©menter un DAG Airflow reprÃ©sentant un pipeline rÃ©el
- Orchestrer les dÃ©pendances entre les tÃ¢ches
- Superviser et diagnostiquer lâ€™exÃ©cution via lâ€™interface Airflow

---

## ğŸ§± Architecture du Pipeline Big Data

### Pipeline logique

```

Sources â†’ Ingestion â†’ Data Lake (RAW) â†’ Traitement â†’ Data Lakehouse (CURATED) â†’ Analytics / BI / IA

```

### Explication
- **Data Lake (RAW)** : stockage des donnÃ©es brutes
- **Traitement** : nettoyage et structuration
- **Data Lakehouse (CURATED)** : donnÃ©es fiables prÃªtes pour lâ€™analyse
- **Airflow** : orchestration et supervision du pipeline

---

## ğŸ§° Technologies utilisÃ©es

- **Apache Airflow 2.8.1**
- **Docker & Docker Compose**
- **PostgreSQL** (base de mÃ©tadonnÃ©es Airflow)
- **Python** (simulation des traitements Big Data)
- **SystÃ¨me de fichiers local** (Data Lake & Lakehouse)

---

## ğŸ“ Structure du projet

```

airflow-bigdata-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ bigdata_pipeline.py
â””â”€â”€ data/
â”œâ”€â”€ raw/
â”œâ”€â”€ processed/
â””â”€â”€ curated/

````

### RÃ´le des dossiers
- `raw/` : Data Lake (donnÃ©es brutes)
- `processed/` : zone intermÃ©diaire
- `curated/` : Data Lakehouse (donnÃ©es prÃªtes pour lâ€™analyse)

---

## ğŸš€ Installation et lancement

### 1ï¸âƒ£ DÃ©marrer les services

```bash
docker-compose up -d
````

### 2ï¸âƒ£ Initialiser Airflow (une seule fois)

```bash
docker-compose run airflow-webserver airflow db init
```

### 3ï¸âƒ£ CrÃ©er lâ€™utilisateur administrateur (une seule fois)

```bash
docker-compose run airflow-webserver airflow users create \
  --username airflow \
  --password airflow \
  --firstname Airflow \
  --lastname Admin \
  --role Admin \
  --email admin@airflow.local
```

### 4ï¸âƒ£ AccÃ©der Ã  lâ€™interface Airflow

* URL : [http://localhost:8080](http://localhost:8080)
* Username : `airflow`
* Password : `airflow`

ğŸ“· *Interface de connexion Airflow*

<img width="3000" height="1119" alt="image" src="https://github.com/user-attachments/assets/18b32d72-116c-410b-a85a-33157bb27200" />


---

## ğŸ§  Description du DAG Big Data

Le DAG `bigdata_pipeline_complete` orchestre les tÃ¢ches suivantes :

1. **Ingestion** : crÃ©ation des donnÃ©es brutes (Data Lake)
2. **Validation** : vÃ©rification de lâ€™existence des donnÃ©es
3. **Transformation** : nettoyage et structuration
4. **Chargement Lakehouse** : donnÃ©es finales pour lâ€™analyse
5. **Analytics** : exploitation mÃ©tier

ğŸ“· *Vue Graph du DAG Airflow*

<img width="2284" height="949" alt="image" src="https://github.com/user-attachments/assets/6c8439d7-e75a-42c8-b6db-920f63ac2bff" />


---

## ğŸ”„ Orchestration et exÃ©cution

### Activer le DAG

* RepÃ©rer `bigdata_pipeline_complete`
* Activer le DAG (bouton ON)

ğŸ“· *Activation du DAG dans Airflow*

<img width="893" height="200" alt="image" src="https://github.com/user-attachments/assets/30a269b7-57d0-4e2d-85c3-667fc363858b" />


### Lancer le pipeline

* Cliquer sur **Trigger DAG**
* Confirmer lâ€™exÃ©cution

ğŸ“· *Lancement manuel du DAG*

<img width="2767" height="125" alt="image" src="https://github.com/user-attachments/assets/04fa28bc-4fc8-49cb-8fea-41288ecd65ef" />


---

## ğŸ“Š Suivi et supervision

### Ã‰tats des tÃ¢ches

* ğŸŸ¢ Vert : succÃ¨s
* ğŸ”´ Rouge : Ã©chec
* ğŸ”µ Bleu : en cours
* âšª Gris : non exÃ©cutÃ©e

ğŸ“· *ExÃ©cution du pipeline en temps rÃ©el*

<img width="2836" height="564" alt="image" src="https://github.com/user-attachments/assets/16ae3b33-5a86-476c-ba98-614daf03efcc" />


### Logs

* Cliquer sur une tÃ¢che
* Ouvrir lâ€™onglet **Log**
* Analyser les messages dâ€™exÃ©cution

ğŸ“· *Logs dâ€™une tÃ¢che Airflow*

> *(InsÃ©rer ici une capture des logs)*

---

## âœ… VÃ©rification des rÃ©sultats

AprÃ¨s exÃ©cution, les fichiers suivants doivent exister :

```
data/raw/sales.csv
data/processed/sales_clean.csv
data/curated/sales_curated.csv
```

ğŸ“· *Structure finale des fichiers*

<img width="2059" height="1095" alt="image" src="https://github.com/user-attachments/assets/2ac1233c-5a70-46dc-9bdb-84605d5cd0fa" />


---

## âš ï¸ Gestion des erreurs

En cas dâ€™Ã©chec :

* consulter les logs,
* corriger le problÃ¨me si nÃ©cessaire,
* cliquer sur **Clear**,
* relancer uniquement la tÃ¢che concernÃ©e.

Cela permet de ne pas redÃ©marrer tout le pipeline.

---

## ğŸ Conclusion

Ce lab illustre concrÃ¨tement :

* la mise en place dâ€™un pipeline Big Data rÃ©aliste,
* lâ€™orchestration avec Apache Airflow,
* la supervision et la gestion des erreurs,
* les bonnes pratiques utilisÃ©es en environnement industriel.

Il constitue une base solide pour Ã©voluer vers des pipelines distribuÃ©s (Spark, Kafka, Cloud).

---

### ğŸ“Œ Auteur

**Zakaria El Guazzar**
Master Intelligence Artificielle
